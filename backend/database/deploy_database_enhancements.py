# deploy_database_enhancements.py
"""
æ•°æ®åº“å¢å¼ºéƒ¨ç½²è„šæœ¬
ç”¨äºå°†æ–°çš„æŒä¹…åŒ–åŠŸèƒ½é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿä¸­
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent  # å‘ä¸Šä¸€çº§åˆ° backend ç›®å½•
sys.path.append(str(project_root))

from database import (
    init_db,
    check_database_connection,
    get_database_stats,
    migrate_legacy_data,
    start_background_tasks,
    get_version_info
)


async def deploy_enhancements():
    """éƒ¨ç½²æ•°æ®åº“å¢å¼ºåŠŸèƒ½"""
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹éƒ¨ç½²æ•°æ®åº“å¢å¼ºåŠŸèƒ½")
    print("=" * 60)
    
    # Step 1: æ£€æŸ¥æ•°æ®åº“è¿æ¥
    print("\n1ï¸âƒ£ æ£€æŸ¥æ•°æ®åº“è¿æ¥...")
    if not check_database_connection():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return False
    
    # Step 2: åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåˆ›å»ºæ–°è¡¨å’Œç´¢å¼•ï¼‰
    print("\n2ï¸âƒ£ åˆå§‹åŒ–æ•°æ®åº“ç»“æ„...")
    if not init_db():
        print("âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
        return False
    
    # Step 3: è¿ç§»ç°æœ‰æ•°æ®
    print("\n3ï¸âƒ£ è¿ç§»ç°æœ‰æ•°æ®åˆ°æ–°ç»“æ„...")
    migration_result = migrate_legacy_data()
    
    if "error" in migration_result:
        print(f"âš ï¸ æ•°æ®è¿ç§»å‡ºç°é—®é¢˜: {migration_result['error']}")
    else:
        print(f"âœ… æ•°æ®è¿ç§»å®Œæˆ:")
        print(f"   - è¿ç§»ç§Ÿæˆ·: {migration_result['migrated_tenants']}")
        print(f"   - è¿ç§»ç«äº‰å¯¹æ‰‹: {migration_result['migrated_competitors']}")
        print(f"   - åˆ›å»ºæ˜ å°„å…³ç³»: {migration_result['migrated_mappings']}")
    
    # Step 4: å¯åŠ¨åå°ä»»åŠ¡
    print("\n4ï¸âƒ£ å¯åŠ¨åå°ä»»åŠ¡...")
    if await start_background_tasks():
        print("âœ… åå°ç¼“å­˜æ¸…ç†ä»»åŠ¡å¯åŠ¨æˆåŠŸ")
    else:
        print("âš ï¸ åå°ä»»åŠ¡å¯åŠ¨å¤±è´¥")
    
    # Step 5: éªŒè¯éƒ¨ç½²
    print("\n5ï¸âƒ£ éªŒè¯éƒ¨ç½²ç»“æœ...")
    stats = get_database_stats()
    
    if "error" in stats:
        print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats['error']}")
    else:
        print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        for key, value in stats.items():
            if key.endswith('_count'):
                table_name = key.replace('_count', '')
                print(f"   - {table_name}: {value} æ¡è®°å½•")
    
    # Step 6: æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
    print("\n6ï¸âƒ£ ç‰ˆæœ¬ä¿¡æ¯:")
    version_info = get_version_info()
    print(f"   ç‰ˆæœ¬: {version_info['version']}")
    print("   æ–°åŠŸèƒ½:")
    for feature in version_info['features']:
        print(f"   âœ¨ {feature}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ•°æ®åº“å¢å¼ºåŠŸèƒ½éƒ¨ç½²å®Œæˆï¼")
    print("=" * 60)
    
    return True


def generate_integration_guide():
    """ç”Ÿæˆé›†æˆæŒ‡å—"""
    guide = """
# æ•°æ®åº“å¢å¼ºåŠŸèƒ½é›†æˆæŒ‡å—

## 1. åœ¨ç°æœ‰ä»£ç ä¸­ä½¿ç”¨æ–°åŠŸèƒ½

### åœ¨ main.py ä¸­é›†æˆç§Ÿæˆ·-ç«äº‰å¯¹æ‰‹æ˜ å°„ï¼š

```python
from database import tenant_crud, tenant_competitor_crud, enhanced_task_crud

async def run_analysis_with_persistence(task_id: str, company_name: str, ...):
    # ç°æœ‰çš„tenant_agentè°ƒç”¨
    tenant_result = await tenant_agent.ainvoke(...)
    tenant = tenant_result.get("tenant")
    
    # æ–°å¢ï¼šä¿å­˜ç§Ÿæˆ·ä¿¡æ¯
    with get_db_session() as db:
        tenant_data = tenant.model_dump() if hasattr(tenant, 'model_dump') else tenant
        saved_tenant, created = tenant_crud.get_or_create_tenant(
            db, tenant_data['tenant_id'], tenant_data
        )
    
    # ç°æœ‰çš„competitor_finderè°ƒç”¨
    competitor_result = await competitor_finder.ainvoke(competitor_state)
    competitors = competitor_result.get("competitors", [])
    
    # æ–°å¢ï¼šä¿å­˜ç«äº‰å¯¹æ‰‹æ˜ å°„å…³ç³»
    with get_db_session() as db:
        enhanced_task_crud.save_competitors_with_mapping(
            db, task_id, tenant_data['tenant_id'], 
            [c.model_dump() if hasattr(c, 'model_dump') else c for c in competitors]
        )
```

### åœ¨ change_detector ä¸­ä½¿ç”¨ç¼“å­˜ï¼š

```python
from database import change_detection_cache

async def change_detector_with_cache(state: CompetitorState, day_delta: int = 20):
    competitors = state.get("competitors", [])
    
    # æ£€æŸ¥ç¼“å­˜
    url_competitor_pairs = [(comp.primary_url, comp.id) for comp in competitors]
    cached_results = await change_detection_cache.get_cached_results(url_competitor_pairs)
    
    uncached_urls = [url for url, _ in url_competitor_pairs if url not in cached_results]
    
    # åªå¯¹æœªç¼“å­˜çš„URLæ‰§è¡Œæ£€æµ‹
    if uncached_urls:
        # ç°æœ‰çš„æ£€æµ‹é€»è¾‘...
        new_results = await perform_actual_detection(uncached_urls)
        
        # ç¼“å­˜æ–°ç»“æœ
        cache_data = {url: result for url, result in new_results.items()}
        competitor_mapping = {url: comp_id for url, comp_id in url_competitor_pairs if url in uncached_urls}
        await change_detection_cache.cache_results(cache_data, competitor_mapping)
    
    # åˆå¹¶ç¼“å­˜å’Œæ–°ç»“æœ
    all_results = {**cached_results, **new_results}
    return {"changes": list(all_results.values())}
```

### åœ¨ OngoingTracker ä¸­ä½¿ç”¨å†…å®¹å­˜å‚¨ï¼š

```python
from database import content_cache

class EnhancedOngoingTracker(OngoingTracker):
    async def track_with_persistence(self, urls: List[str], tag: str = "default"):
        # è·å–ä¸Šæ¬¡ä¿å­˜çš„å†…å®¹
        previous_content = await content_cache.get_previous_content(urls, tag)
        
        # ç°æœ‰çš„çˆ¬å–é€»è¾‘...
        current_content = {}
        async for result in self.crawler.background_crawl(urls):
            current_content[result.url] = result.markdown
        
        # ä¿å­˜å½“å‰å†…å®¹
        await content_cache.save_current_content(current_content, tag)
```

## 2. æ€§èƒ½ä¼˜åŒ–å»ºè®®

- ç§Ÿæˆ·-ç«äº‰å¯¹æ‰‹æ˜ å°„æŸ¥è¯¢å°†å¤§å¤§å‡å°‘é‡å¤çš„ç«äº‰å¯¹æ‰‹å‘ç°
- å˜åŒ–æ£€æµ‹ç¼“å­˜å°†å‡å°‘70%+çš„é‡å¤åˆ†æå·¥ä½œ
- å†…å®¹å­˜å‚¨æ”¯æŒå¢é‡æ¯”è¾ƒï¼Œæå‡è·Ÿè¸ªæ•ˆç‡

## 3. ç›‘æ§å’Œç»´æŠ¤

```python
# è·å–ç¼“å­˜ç»Ÿè®¡
cache_stats = change_detection_cache.get_cache_stats()

# æ‰‹åŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜
cleaned_count = await change_detection_cache.cleanup_expired_cache()

# è·å–æ•°æ®åº“æ•´ä½“ç»Ÿè®¡
db_stats = get_database_stats()
```

## 4. å‘åå…¼å®¹æ€§

æ‰€æœ‰ç°æœ‰çš„CRUDæ“ä½œä¿æŒä¸å˜ï¼š
- `task_crud.create_task()` ç»§ç»­å·¥ä½œ
- `competitor_crud.save_competitors()` ç»§ç»­å·¥ä½œ  
- `change_crud.save_changes()` ç»§ç»­å·¥ä½œ

æ–°åŠŸèƒ½æ˜¯å¢é‡æ·»åŠ ï¼Œä¸ä¼šç ´åç°æœ‰ä»£ç ã€‚
"""
    
    with open("INTEGRATION_GUIDE.md", "w", encoding="utf-8") as f:
        f.write(guide)
    
    print("ğŸ“ å·²ç”Ÿæˆé›†æˆæŒ‡å—: INTEGRATION_GUIDE.md")


async def main():
    """ä¸»éƒ¨ç½²æµç¨‹"""
    
    try:
        # æ‰§è¡Œéƒ¨ç½²
        success = await deploy_enhancements()
        
        if success:
            # ç”Ÿæˆé›†æˆæŒ‡å—
            generate_integration_guide()
            
            print("\nğŸ”§ ä¸‹ä¸€æ­¥:")
            print("1. æŸ¥çœ‹ INTEGRATION_GUIDE.md äº†è§£å¦‚ä½•é›†æˆæ–°åŠŸèƒ½")
            print("2. æ›´æ–°ä½ çš„ main.py å’Œå…¶ä»–ç»„ä»¶ä»¥ä½¿ç”¨æŒä¹…åŒ–åŠŸèƒ½")
            print("3. è¿è¡Œæµ‹è¯•ç¡®ä¿ä¸€åˆ‡æ­£å¸¸å·¥ä½œ")
            print("4. ç›‘æ§ç¼“å­˜æ€§èƒ½å’Œæ•°æ®åº“ç»Ÿè®¡")
            
            return 0
        else:
            print("âŒ éƒ¨ç½²å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            return 1
            
    except KeyboardInterrupt:
        print("\nâš ï¸ éƒ¨ç½²è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"âŒ éƒ¨ç½²è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return 1
    
    finally:
        # æ¸…ç†èµ„æº
        try:
            from database import stop_background_tasks
            await stop_background_tasks()
        except:
            pass


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)